{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Henry PI 2: Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omarly Zerpa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Analisis Exploratorio de Datos) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se extraen las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se importa el archivo tipo parquet con los datos de entrenamiento a utilizar en un Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet= r'train.parquet'\n",
    "df= pd.read_parquet(parquet, engine='auto')\n",
    "df.head(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos informacion basica del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos registros duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos si existen valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificaremos la columna long (longitud) y lat (latitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buscamos los valores unicos\n",
    "df['lat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['long'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un nuevo dataframe para examinar mas de cerca si los datos de  lat y la long pertenecen a la region y estado indicado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latitud_longitud= df[['region','state', 'lat', 'long']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos que que regiones tienen laitud y logitud nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latitud_longitud[(df_latitud_longitud['lat'].isnull())]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinanos los valores de latitud y longitud de una region en especifico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latitud_longitud[(df_latitud_longitud['region']=='denver')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evidencia que cada region cuenta con una latitud y longitud especifica imposibilitando el cambio a un valor unico, ya que no se brindaria la ubicacion exacta del inmueble. Ademas desconozco el criterio empleado con dichos valores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, se calcula el porcentaje de nulos en la columna latitud y longitud (es la misma cantidad) para evidenciar el impacto que tiene sobre el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos= df['lat'].isnull().sum()\n",
    "total= len(df['lat'])\n",
    "porcentaje= ((nulos/ total)*100)\n",
    "print('El porcentaje de nulos en la columna lat', porcentaje)# es irrelevante para hacer una eliminacion de las columnas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se renombran las columnas lat y long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'lat':'latitude', 'long':'longitude'}, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se confirma el cambio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos con dispersion la latitude y longitude, para confirmar que se encuentran en el pais indicado(EEUU) por lo general es: Estándar decimal simple\tlatitud:37.09024, longitud:-95.712891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, figsize=(10,7))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen puntos que se encuentran afueras de los limites de los EEUU, por esta razon no se tomara en cuenta estas columnas para el analisis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analicemos las columnas laundry_options y parking_options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos= df['laundry_options'].isnull().sum()\n",
    "total= len(df['laundry_options'])\n",
    "porcentaje= ((nulos/ total)*100)\n",
    "print('El porcentaje de nulos en la columna laundry_options', porcentaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos= df['parking_options'].isnull().sum()\n",
    "total= len(df['parking_options'])\n",
    "porcentaje= ((nulos/ total)*100)\n",
    "print('El porcentaje de nulos en la columna parking_options', porcentaje)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observamos representa un valor mayor al 10% de valores faltantes, podrian ser datos importantes como caracteristica del inmueble si no tuvise valores perdidos o faltantes, ahora para el estudio de este caso se obviaran estas columnas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se eliminan las columnas mencionadas\n",
    "df.drop(['laundry_options','parking_options'], axis= 1,  inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los atribulos no seleccionados para este analisis son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id', 'url', 'region_url', 'image_url', 'latitude', 'longitude'], axis= 1,  inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se confirman los cambios realizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa la informacion de la columna type, para saber si requiere algun cambio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = \"type\", data = df, kind = \"count\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, se crea la columna category_price utilizando la columna price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores= [-math.inf, 999, 1999,math.inf]\n",
    "category= ['low','medium','high']\n",
    "df2= df.assign(category_price= pd.cut(x=df['price'], bins=valores, labels=category, include_lowest=True))\n",
    "df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se grafica los datos de la columna creada para visualizar la cantidad de inmubles que existe con price low, medium y high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='category_price', data=df2) #equivelante a sns.countplot(df.Conversion2)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una columna target la cual me indique 1 para los inmuebles con price low y 0 para los inmuebles con price medium y high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['target'] = df2['category_price'].apply(lambda x : 1 if x=='low' else 0)\n",
    "df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifican los cambios realizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlaciones de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crea una lista con los atributos de los inmuebles\n",
    "atributos= df2[['sqfeet','beds','baths', 'cats_allowed', 'dogs_allowed', 'smoking_allowed','wheelchair_access', 'electric_vehicle_charge','comes_furnished','target', 'price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corre_heatmap(corr):\n",
    "    '''\n",
    "    Definimos una función para ayudarnos a graficar un heatmap de correlación\n",
    "    '''\n",
    "    plt.figure(figsize=(12,12))\n",
    "    sns.heatmap(corr, cbar = True,  square = False, annot=True, fmt= '.2f'\n",
    "                ,annot_kws={'size': 15},cmap= 'coolwarm')\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.yticks(rotation = 45)\n",
    "    # Arreglamos un pequeño problema de visualización\n",
    "    b, t = plt.ylim() # descubre los valores para bottom y top\n",
    "    b += 0.5 # Agrega 0.5 al bottom\n",
    "    t -= 0.5 # Resta 0.5 de la parte de top\n",
    "    plt.ylim(b, t) # actualizar los valores  ylim(bottom, top)\n",
    "    plt.show()\n",
    "\n",
    "corr = atributos.corr()\n",
    "plot_corre_heatmap(corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino mi tabla train a utilizar para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= df2 \n",
    "train.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado la cantidad de nulos y valores no pertenecientes las columnas latitude y longitude,y escasa correlaciones entre las caracteristicas y el precio del inmueble, tomé la descripción para obtener palabras reelevantes que me permitan deducir si un inmueble tiene precio bajo o alto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos si existen descripciones faltantes en la columna descripción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['description'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las descripciones vacías porque mejora la performance del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(subset='description', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos que no hayan quedado registros con descripciones vacías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['description'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de datos utilizando la columna descripcion y target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos los modulos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los stop words de esta biblioteca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con la lista de los textos de las descripciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train['description']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo convertimos a lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el vectorizador con los stop words y un máximo de features de 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=sw, max_features=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos la cantidad de datos obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension de la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline utilizando la columna descripcion y target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([('rf', RandomForestClassifier(n_estimators=50, random_state=0)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf.fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el Pipeline en un archivo Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipe_rf, 'modelo_random_forest2.pkl', compress=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el archivo creado de nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = joblib.load('modelo_random_forest2.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos el umbral de puntuacion (Adjust Score Threshold): Ajustar este valor cambia el nivel de confianza que debe tener el modelo en una predicción antes de considerar que la predicción es positiva. También cambia la cantidad de falsos negativos y falsos positivos que está dispuesto a tolerar en sus predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(rf.predict_proba(x_test) [:,1] > 0.2, 1, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una matriz de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=rf.classes_)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=rf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinados la exhaustividad(recall), la precision y la exactitud(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = recall_score(y_test, y_pred)\n",
    "ps = precision_score(y_test, y_pred)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Recall: {rs}')\n",
    "print(f'Precision: {ps}')\n",
    "print(f'Accuracy: {ac}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traemos el archivo test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet= r'test.parquet'\n",
    "df_t= pd.read_parquet(parquet, engine='auto')\n",
    "df_t.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos las mismas transformaciones que realizamos para el train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renombramos las columnas\n",
    "df_t.rename(columns={'lat':'latitude', 'long':'longitude'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos los columanas por las razones explicadas con anterioridad\n",
    "df_t.drop(['id', 'url', 'region_url', 'laundry_options', 'parking_options', 'image_url','latitude', 'longitude'], axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la columna category_price\n",
    "valores2= [-math.inf, 999, 1999,math.inf]\n",
    "category2= ['low','medium','high']\n",
    "df_t2= df.assign(category_price= pd.cut(x=df['price'], bins=valores, labels=category, include_lowest=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la columna target\n",
    "df_t2['target']= df_t2['category_price'].apply(lambda x:1 if x=='low' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se define la tabla test ha utilizar\n",
    "test= df_t2\n",
    "test.head(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos la columna description para utilizarla "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos descripciones faltantes en la columna descripción\n",
    "test['description'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los índices de los registros con descripciones vacías\n",
    "sin_desc = test[test.description.isnull()].index\n",
    "sin_desc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una funcion para para rellenar la descripciones faltantes en la columna description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearDescription(df, num_loc):\n",
    "    '''\n",
    "    Arma la descripción faltante con datos existentes\n",
    "\n",
    "    df: DataFrame a analizar\n",
    "    num_loc: índice del registro a cambiar\n",
    "    '''\n",
    "    cadena = ''\n",
    "    # Evaluamos con que características contamos para armar\n",
    "    # las descripciónes faltantes de las propiedades\n",
    "    if not df.isnull().type[num_loc]: \n",
    "            cadena += df.type[num_loc] + ' '\n",
    "    if not df.isnull().region[num_loc]:\n",
    "        cadena += df.region[num_loc] + ' '\n",
    "    if not df.isnull().beds[num_loc]:\n",
    "        cadena += str(int(df.beds[num_loc])) + ' habitaciones '\n",
    "    if not df.isnull().baths[num_loc]:\n",
    "        cadena += str(int(df.baths[num_loc])) + ' baños'\n",
    "    \n",
    "    return cadena"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, recorremos los índices de los registros con descripciones faltantes para completarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in sin_desc:\n",
    "    test.loc[idx,('description')] = crearDescription(test, idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que todas las propiedades tengan una descripción\n",
    "test['description'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los stop words de esta biblioteca \n",
    "sw = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con la lista de los textos de las descripciones\n",
    "corpus_t = test['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo convertimos a lista\n",
    "corpus_t = list(corpus_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos el tamaño de los datos obtendos, para observar que coincide \n",
    "# con la cantidad de registros a predecir\n",
    "len(corpus_t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la modelizacion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos el vectorizador con los stop words y un máximo de features de 1000\n",
    "vectorizer_t = TfidfVectorizer(stop_words=sw, max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizamos los datos\n",
    "x_t = vectorizer_t.fit_transform(corpus_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observamos la cantidad de datos obtenidos\n",
    "x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajustamos el umbral de puntuacion (Adjust Score Threshold)\n",
    "y_t_predict = np.where(rf.predict_proba(x_t) [:,1] > 0.2, 1, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación del CSV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el DataFrame que contendrá las predicciones obtenidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.DataFrame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la columna pred con los datos obtendiso de la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['pred'] = y_t_predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo guardamos con formato CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.to_csv('OmarlyZerpa.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e8c5f23fb510477d5fd12bb1fd390b47d8bdccc66b73a6f44117f30d4426fae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
